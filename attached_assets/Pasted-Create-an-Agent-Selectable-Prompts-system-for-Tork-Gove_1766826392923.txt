Create an Agent-Selectable Prompts system for Tork Governance SDK.

REQUIREMENTS:

1. Create src/tork/prompts/__init__.py with exports

2. Create src/tork/prompts/models.py:
   
   class PromptType(str, Enum):
       """Types of prompts."""
       SYSTEM = "system"
       USER = "user"
       ASSISTANT = "assistant"
       CRITIQUE = "critique"
       SYNTHESIS = "synthesis"
       REFINEMENT = "refinement"
       EXPANSION = "expansion"
       COMPRESSION = "compression"
   
   class PromptQuality(str, Enum):
       """Quality assessment of a prompt."""
       EXCELLENT = "excellent"
       GOOD = "good"
       ACCEPTABLE = "acceptable"
       POOR = "poor"
       REJECTED = "rejected"
   
   class PromptCandidate(BaseModel):
       """A candidate prompt generated by an agent."""
       id: str = Field(default_factory=lambda: str(uuid4()))
       prompt_type: PromptType
       content: str
       generator_agent: str      # Agent ID that generated this
       generator_model: str = "" # Model used
       
       # Quality metrics
       quality: PromptQuality = PromptQuality.ACCEPTABLE
       clarity_score: float = 0.5      # 0-1
       specificity_score: float = 0.5  # 0-1
       safety_score: float = 1.0       # 0-1
       
       # Metadata
       token_count: int = 0
       generation_time_ms: int = 0
       metadata: Dict[str, Any] = {}
   
   class PromptSelectionCriteria(BaseModel):
       """Criteria for selecting the best prompt."""
       min_clarity: float = 0.5
       min_specificity: float = 0.5
       min_safety: float = 0.8
       max_tokens: int = 4096
       preferred_generators: List[str] = []  # Agent IDs
       blocked_generators: List[str] = []
       prefer_quality: PromptQuality = PromptQuality.GOOD
   
   class PromptSelectionResult(BaseModel):
       """Result of prompt selection."""
       selected: PromptCandidate
       candidates: List[PromptCandidate]
       selection_reasoning: str
       confidence: float = 1.0

3. Create src/tork/prompts/generator.py:
   
   class PromptGenerator:
       """Generate prompts using specified agents."""
       
       def __init__(self, governance_engine: GovernanceEngine = None):
           self._governance_engine = governance_engine
           self._executors: Dict[str, Callable] = {}
       
       def register_executor(self, agent_id: str, executor: Callable):
           """Register an executor for an agent."""
       
       def generate(self, 
                   task: str, 
                   prompt_type: PromptType,
                   agent_id: str,
                   context: str = "") -> PromptCandidate:
           """Generate a prompt using specified agent."""
           - Build meta-prompt asking agent to generate a prompt
           - Execute via registered executor
           - Apply governance to output
           - Score the generated prompt
           - Return PromptCandidate
       
       def generate_multiple(self,
                            task: str,
                            prompt_type: PromptType,
                            agent_ids: List[str],
                            context: str = "") -> List[PromptCandidate]:
           """Generate prompts from multiple agents."""
       
       def _score_prompt(self, content: str) -> Dict[str, float]:
           """Score a prompt for clarity, specificity, safety."""
       
       def _assess_quality(self, scores: Dict[str, float]) -> PromptQuality:
           """Assess overall quality from scores."""

4. Create src/tork/prompts/selector.py:
   
   class PromptSelector:
       """Select the best prompt from candidates."""
       
       def __init__(self, governance_engine: GovernanceEngine = None):
           self._governance_engine = governance_engine
       
       def select(self, 
                 candidates: List[PromptCandidate],
                 criteria: PromptSelectionCriteria = None) -> PromptSelectionResult:
           """Select the best prompt based on criteria."""
           - Filter by safety minimum
           - Filter by blocked generators
           - Score remaining by clarity, specificity
           - Apply preference weights
           - Return best with reasoning
       
       def rank(self, 
               candidates: List[PromptCandidate],
               criteria: PromptSelectionCriteria = None) -> List[PromptCandidate]:
           """Rank all candidates by quality."""
       
       def _calculate_score(self, candidate: PromptCandidate, criteria: PromptSelectionCriteria) -> float:
           """Calculate weighted score for a candidate."""

5. Create src/tork/prompts/orchestrator.py:
   
   class PromptOrchestrator:
       """Orchestrate multi-agent prompt generation and selection."""
       
       def __init__(self, generator: PromptGenerator = None, selector: PromptSelector = None):
           self._generator = generator or PromptGenerator()
           self._selector = selector or PromptSelector()
       
       def orchestrate(self,
                      task: str,
                      prompt_type: PromptType,
                      agent_ids: List[str],
                      criteria: PromptSelectionCriteria = None,
                      context: str = "") -> PromptSelectionResult:
           """Generate prompts from multiple agents and select the best."""
       
       def refine(self,
                 prompt: PromptCandidate,
                 refiner_agent: str,
                 feedback: str = "") -> PromptCandidate:
           """Refine a prompt using another agent."""
       
       def iterate(self,
                  task: str,
                  agent_ids: List[str],
                  max_iterations: int = 3,
                  criteria: PromptSelectionCriteria = None) -> PromptSelectionResult:
           """Iteratively generate and refine until quality threshold met."""

6. Create src/tork/prompts/templates.py:
   
   Meta-prompt templates for different prompt types:
   
   def critique_meta_prompt(task: str, context: str = "") -> str:
       """Meta-prompt for generating critique prompts."""
   
   def synthesis_meta_prompt(task: str, context: str = "") -> str:
       """Meta-prompt for generating synthesis prompts."""
   
   def refinement_meta_prompt(original: str, feedback: str = "") -> str:
       """Meta-prompt for refining prompts."""
   
   def expansion_meta_prompt(task: str, context: str = "") -> str:
       """Meta-prompt for expanding brief prompts."""
   
   def compression_meta_prompt(task: str, context: str = "") -> str:
       """Meta-prompt for compressing verbose prompts."""

7. Create tests/test_prompts.py with 22+ tests:
   - Test PromptType enum
   - Test PromptQuality enum
   - Test PromptCandidate model
   - Test PromptSelectionCriteria model
   - Test PromptSelectionResult model
   - Test PromptGenerator initialization
   - Test PromptGenerator generate
   - Test PromptGenerator generate_multiple
   - Test prompt scoring
   - Test quality assessment
   - Test PromptSelector select
   - Test PromptSelector with criteria
   - Test PromptSelector rank
   - Test blocked generators filtering
   - Test safety threshold
   - Test PromptOrchestrator orchestrate
   - Test PromptOrchestrator refine
   - Test PromptOrchestrator iterate
   - Test meta-prompt templates
   - Test governance integration

8. Update src/tork/__init__.py to export prompts classes

Run: python -m pytest tests/test_prompts.py -v